#!/usr/bin/env python
# License: Apache 2.0
"""Listen for connections.

When it receives data, shove the data into a postgis server.

Expects normalized AIS messages.

nohup ./ais-net-to-postgis -a 10.0.3.5 -a 208.77.188.166 -a 127.0.0.1 \
  -I 0.0.0.0 -s "3 hours ago" -S "4 hours ago" -c 600 &

TODO(schwehr): Detect when one of the child threads dies from an exception.
TODO(schwehr): Allow the db to get synced on startup, so we do not have to
  wait for the first vessel traffic.
TODO(schwehr): Perhaps also periodically rebuild the cache if nothing going on
  if there are cache entries so that they can expire.
"""

import datetime
import exceptions
import logging
import select
import socket
import sys
import thread
import time
import traceback

import magicdate

import ais
import aisutils.sqlhelp
import aisutils.database
import aisutils.uscg


class DatabaseHandler:
    """Queue handling for the database."""
    def __init__(self,
                 connection,
                 dbUpdateInterval=5.0,
                 threshold=1,
                 verbose=False,
                 skipDB=False,
                 limitPoints=10,
                 track_start_time_limit=None,  # 1 hour ago
                 last_position_time_limit=None,  # 6 hours ago
                 cleanTime=30):
        """
        @param connection: database connection
        @param dbUpdateInterval: how many seconds between database updates
        @param threshold: How many messages in the queue before they go to the db?
        @param skipDB: Do not actually talk to the database.  For debugging.
        @param limitPoints: max number of points in a ship track.  None for no limit
        @param : a string limiting the length of ship tracks wrt time.  For example: "6 hours ago".  None for no limit
        @param : a string limiting the length of ship tracks wrt time.  For example: "6 hours ago".  None for no limit
        @param cleanTime: seconds between running the database cleanup for the track lines
        """
        if not skipDB:
            self.cx = connection
            self.cu = connection.cursor()
        else:
            self.cx = None
            self.cu = None
        import Queue
        self.q = Queue.Queue()
        self.running = True
        self.dbUpdateInterval = dbUpdateInterval
        self.threshold = threshold
        self.verbose = verbose
        # Set to true when this class is all done and committed.
        self.stopped = False
        self.skipDB = skipDB

        self.limitPoints = limitPoints
        self.track_start_time_limit = track_start_time_limit
        self.last_position_time_limit = last_position_time_limit
        self.cleanTime = cleanTime
        if self.verbose:
            logging.info('Database handler init.')
            logging.info('  track_start: %s', self.track_start_time_limit)
            logging.info('  last_position: %s', self.last_position_time_limit)

    def stop(self):
        self.running = False
        if self.verbose:
            logging.info('Database handler stop scheduled')

    def commit(self):
        """Slow loop that commits groups of position reports to the database.

        @todo: Build the lines/shiptracks from the position reports for the ships with new positions.
        @todo: Where do I decimate reports received from multiple receivers?
        @todo: Need to track the ships in the put.
        """
        q = self.q
        cx = self.cx
        cu = self.cu
        skipDB = self.skipDB
        vesselsSeen = set()

        size = q.qsize()
        if size >= self.threshold:
            # Don't try to flush incoming messages.
            if self.verbose:
                logging.info('Pulling from queue.  size: %d', size)
            for i in range(size):
                sqlStr, vessel = q.get()
                if vessel is not None:
                    vesselsSeen.add(vessel)
                if self.verbose:
                    logging.info('exec in commit: %d %s', i, sqlStr)
                if not skipDB:
                    try:
                        cu.execute(sqlStr)
                    except Exception, e:
                        logging.error('Exception on sql: %s, ', sqlStr)
                        logging.error('   Exception: %s', type(Exception))
                        logging.error('   Exception args: %s', e)
                        traceback.print_exc(file=sys.stderr)
                        continue
            if skipDB:
                logging.info('Skipping commit.')
            else:
                if self.verbose:
                    logging.info('Committing.')
                cx.commit()
                if self.verbose:
                    logging.info('Recalculate ship tracks vessels seen.')
                    logging.info('  '+str(vesselsSeen)+'\n')
                if len(vesselsSeen) > 0:
                    # Always work in UTC.  Magicdate is local.
                    tzoffset = datetime.timedelta(seconds=time.timezone)

                    track_start_time = magicdate.magicdate(
                        self.track_start_time_limit) + tzoffset
                    aisutils.database.rebuild_track_lines(
                        cx, vessels=vesselsSeen, startTime=track_start_time,
                        verbose=self.verbose)

                    last_pos_start_time = magicdate.magicdate(
                        self.last_position_time_limit) + tzoffset
                    aisutils.database.rebuild_last_position(
                        cx,
                        vesselsClassA=vesselsSeen,
                        startTime=last_pos_start_time,
                        verbose=self.verbose)

    def clean(self):
        """Run through the vessel list so that aging works."""

        if (self.track_start_time_limit is None
            and self.last_position_time_limit is None):
            # No time expirations, so don't clean.
            return

        # Do this everytime to make sure daylight savings does not nail us.
        tzoffset = datetime.timedelta(seconds=time.timezone)

        if self.verbose:
            logging.info('Cleaning.  utcnow: %s', datetime.datetime.utcnow())

        if self.track_start_time_limit:
            startTime = magicdate.magicdate(
                self.track_start_time_limit) + tzoffset
            if self.verbose:
                logging.info('Cleaning track_start %s (%s)',
                             startTime, self.track_start_time_limit)

            aisutils.database.rebuild_track_lines(self.cx,
                                                  startTime=startTime,
                                                  verbose=self.verbose)
            logging.info('Done cleaning track_start.')

        if self.last_position_time_limit:
            startTime = magicdate.magicdate(
                self.last_position_time_limit) + tzoffset
            if self.verbose:
                logging.info('Clean last_position %s (%s)',
                             startTime, self.last_position_time_limit)
            aisutils.database.rebuild_last_position(
                self.cx, startTime=startTime, verbose=self.verbose)
        if self.verbose:
            logging.info('Done cleaning.')

    def handler(self, unused=None):
        """Top level loop thread."""
         # Do an immediate clean
        nextClean = datetime.datetime.utcnow()

        while self.running:
            time.sleep(self.dbUpdateInterval)
            try:
                self.commit()
            except Exception as e:
                logging.info('Exception args: %s', e)
                traceback.print_exc(file=sys.stderr)
                continue

            if datetime.datetime.utcnow() > nextClean:
                self.clean()
                nextClean = (
                    datetime.datetime.utcnow() +
                    datetime.timedelta(seconds=self.cleanTime))

        logging.info('Database handler stopping. Begin final commit.')
        self.commit()
        self.stopped = True


class HandleAisConnection:
    """Handles the incoming socket."""

    def __init__(self, dataSocket, dbQueue, options, dbType='postgres'):
        """
        @param dbQueue: Queue object to push SQL messages onto
        """
        self.options = options
        logging.info('hack options in __init__: %s', self.options)
        self.dataSocket = dataSocket
        self.running = True
        self.dbQueue = dbQueue
        self.dbType = dbType
        try:
            self.timeout = options.timeout
        except:
            self.timeout = 1.
        self.buf = None

    def handler(self, unused=None):
        v = self.options.verbose
        if v:
            logging.info('Handler started.')

        while self.running:
            readersready, outputready, exceptready = select.select([self.dataSocket,],[],[],self.timeout)
            for sock in readersready:
                data = sock.recv(1024)

                if len(data) == 0:
                    # Socket is closed.
                    if v:
                        logging.info('Shutting down ais connection handler\n')
                    self.running = False
                    continue

                if self.buf is not None:
                    data = self.buf + data
                    if v:
                        logging.info('new buf: %s', data)
                    self.buf = None
                nmeaStrs = data.split('\n')
                if data[-1] not in ('\n','\r'):
                    self.buf = nmeaStrs.pop()

                if len(data) == 0:
                    continue

                for msg in nmeaStrs:
                    if len(msg) == 0:
                        continue
                    if not msg.startswith('!AIVDM'):
                        if v:
                            logging.info('Skipping non ais message "'+msg+'"\n')
                        continue
                    else:
                        if v:
                            logging.info('processing ais message ... '+msg+'\n')
                        pass

                    uscgMsg = aisutils.uscg.UscgNmea(msg)

                    if uscgMsg.totalSentences != 1:
                        if v:
                            logging.info('Skip un-normalized messages.')
                            logging.info('  msg: %s', msg)
                        continue

                    if uscgMsg.msgTypeChar not in ais.msgModByFirstChar:
                        if v:
                            logging.info('Unhandled message: %s',
                                         uscgMsg.msgTypeChar)
                        continue
                    aismsg = ais.msgModByFirstChar[uscgMsg.msgTypeChar]
                    bv = ais.binary.ais6tobitvec(uscgMsg.contents)
                    try:
                        msgDict = aismsg.decode(bv)
                    except Exception as e:
                        logging.info('   Dropping bad msg: %s', e)
                        continue

                    ins = aismsg.sqlInsert(msgDict, dbType=self.dbType)

                    cg_sec = uscgMsg.cg_sec
                    cg_timestamp = uscgMsg.sqlTimestampStr
                    cg_station = uscgMsg.station

                    if cg_sec is not None:
                        ins.add('cg_sec', cg_sec)
                    if cg_timestamp is not None:
                        ins.add('cg_timestamp', cg_timestamp)
                    if cg_station is not None:
                        ins.add('cg_r', cg_station)

                    # Figure out if the vessel track needs to be updated
                    vessel = None
                    if aismsg.dbTableName in ['position']:
                        vessel = msgDict['UserID']
                    self.dbQueue.put((str(ins), vessel))


class PassThroughServer:
    """Receive data from a socket and write the data to all clients.

    Starts two threads and returns to the caller.
    """
     # Indexed by socket. Handles if we have partial text.
    nmeaInputs = {}
    def __init__(self, options, dbHandler):
        """
        @param options: understands timeout (float in seconds)
        """
        self.clients = []
        self.options = options
        self.count = 0
        try:
            self.timeout = options.timeout
        except:
            self.timeout = 1.
        try:
            self.hosts_allow = options.hosts_allow
        except:
            self.hosts_allow = None
        self.running = True
        self.hacs = []
        self.dbHandler = dbHandler

    def start(self):
        """Start the thread.

        TODO(schwehr): Cleanup and use verbose flag.
        """
        logging.info('Starting threads.\n')
        thread.start_new_thread(self._connection_handler, (self,))
        logging.info('connection_handler started\n')
        return


    def _connection_handler(self, unused=None):
        """Listen for connections and add to clients.

        Do not use this.  Call start() instead.

        @bug: how can I get rid of unused?
        """
        inHost = self.options.inHost
        inPort = self.options.inPort
        logging.info('Starting connection receiver: %s:%s', inHost, inPort)

        serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        serversocket.bind((inHost, inPort))
        serversocket.listen(5)

        while self.running:
            (clientsocket, address) = serversocket.accept()
            logging.info('connect from %s %s', clientsocket, address)
            if (self.hosts_allow is not None
                and address[0] not in self.hosts_allow):
                logging.info('Dropping connection from: %s', address)
                continue
            else:
                logging.info('Creating handler.')
                hac = HandleAisConnection(clientsocket,
                                          self.dbHandler.q,
                                          self.options,
                                          self.options.dbType)
                logging.info('Creating thread')
                thread.start_new_thread(hac.handler, (self,))
                self.hacs.append(hac)

        # TODO(schwehr): Graceful shutdown.
        # Stop all handlers.
        # Close and flush the database handler

    def stop(self):
        self.running = False

def main():
    from optparse import OptionParser

    parser = OptionParser(
        usage='%prog [options]',
        version='%prog')

    parser.add_option(
        '-i', '--in-port',
        dest='inPort',
        type='int',
        default=31402,
        help='Where the data comes from [default: %default]')

    parser.add_option(
        '-I', '--in-host',
        dest='inHost',
        type='string',
        default='localhost',
        help='What host to read data from [default: %default]')

    parser.add_option(
        '--in-gethostname',
        dest='inHostname',
        action='store_true',
        default=False,
        help='Where the data comes from [default: %default]')

    parser.add_option(
        '-a', '--allow',
        action='append',
        dest='hosts_allow',
        help='Add hosts to a list that are allowed to connect [default: all]')

    parser.add_option(
        '-t', '--timeout',
        dest='timeout',
        type='float',
        default='300',
        help='Number of seconds to timeout if no data [default: %default]')

    aisutils.database.stdCmdlineOptions(parser, 'postgres')

    parser.add_option(
        '-v', '--verbose',
        dest='verbose',
        default=False,
        action='store_true',
        help='Make the test output verbose')

    parser.add_option(
        '--dummy-db',
        dest='skipDB',
        default=False,
        action='store_true',
        help='Do not actually talk to database')

    parser.add_option(
        '--time-limit-all',
        dest='timeLimitAll',
        type='str',
        default=None,
        help='Limit all caches by one magic date time (e.g. "1 hour ago")'
             '-s and -S override this [default %default]')

    # These will get evaluated each time around the loop.
    parser.add_option(
        '-s', '--track-start-time',
        dest='track_start',
        type='str',
        default=None,
        help='magicdate - Oldest allowable time for a track line '
             '[default %default]')

    parser.add_option(
        '-S', '--last-position-start-time',
        dest='last_position_start',
        type='str',
        default=None,
        help='magicdate - Oldest allowable time for a last position '
             '[default %default]')

    parser.add_option(
        '-c', '--clean-time',
        dest='cleanTime',
        type='float',
        default=30,
        help='Time in seconds between database cleanup of the track lines '
             '[default %default]')

    options, args = parser.parse_args()

    v = options.verbose
    # TODO(schwehr): Don't force this.
    options.dbType = 'postgres'

    if options.timeLimitAll is not None:
        if options.track_start is None:
            options.track_start = options.timeLimitAll
        if options.last_position_start is None:
            options.last_position_start = options.timeLimitAll

    if options.track_start is not None:
        try:
            m = magicdate.magicdate(options.track_start)
        except Exception:
            sys.exit('ERROR: track_start not a valid magic date: '
                     + options.track_start)
        if m is None:
            sys.exit('track_start is not a valid magic date: '
                     + options.track_start)
    if options.last_position_start is not None:
        try:
            m = magicdate.magicdate(options.last_position_start)
        except Exception:
            sys.exit('PARSE ERROR: last_position_start not a valid magic date: '
                     + options.last_position_start)
        if m is None:
            sys.exit('last_position_start not a valid magic date: '
                     + options.last_position_start)

    cx = None
    if not options.skipDB:
        cx = aisutils.database.connect(options, dbType=options.dbType)


    logging.info('Creating dbHandler verbosity = %s', v)
    dbHandler = DatabaseHandler(
        cx,
        track_start_time_limit=options.track_start,
        last_position_time_limit=options.last_position_start,
        verbose=v,
        skipDB=options.skipDB,
        cleanTime=options.cleanTime)

    if v:
        logging.info('hosts allowed: %s ', options.hosts_allow)

    if options.inHostname:
        options.inHost = socket.gethostname()

    pts = PassThroughServer(options, dbHandler)
    pts.start()

    # Now start up the thread to send the messages to
    thread.start_new_thread(dbHandler.handler, (None, ))

    timeout = options.timeout
    i = 0
    running = True

    try:
        while running:
            i += 1
            time.sleep(timeout)
            if v:
                logging.info('ping %d', i)
    except KeyboardInterrupt:
        if v:
            logging.info('Shutting down.')

    # TODO(schwehr): Nuke all the connections?

    dbHandler.stop()
    while not dbHandler.stopped:
        time.sleep(.1)
    if v:
        logging.info('Finished cleaning up.  Goodbye.')


if __name__ == '__main__':
    main()
